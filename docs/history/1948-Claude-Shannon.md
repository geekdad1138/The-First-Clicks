## **Claude Shannon: A Mathematical Theory of Communication (1948)**

### **What problem existed?**

Before 1948, "information" was a vague, subjective concept. Engineers struggled with how to transmit messages efficiently over noisy channels (like telegraph or telephone lines) without losing meaning. There was no universal way to measure information, no way to know the maximum speed of a connection, and no understanding of how to "compress" data.

### **What did this person actually do?**

Shannon defined information as the **reduction of uncertainty**. He introduced the **bit** (binary digit) as the fundamental unit of information and proved that any message—whether text, sound, or image—could be encoded into 1s and 0s. He also calculated the "Channel Capacity" (), the absolute speed limit for data transmission over any medium.

### **Why did it unlock something?**

It turned communication from a hit-or-miss engineering hurdle into a precise branch of physics. By separating the *content* of a message from its *physical medium*, Shannon made it possible to treat data as a commodity. It gave us the math for **Error Correction**: the ability to send a message through static and have it arrive perfectly on the other side.

### **What wouldn't exist without it?**

* **The Internet:** Every packet of data sent over TCP/IP relies on Shannon’s theories of encoding and transmission.
* **Data Compression:** From ZIP files to JPEGs to the weights of an LLM, the idea that we can represent complex data with fewer bits comes directly from his concept of "Entropy."
* **Modern AI:** LLMs are essentially massive prediction engines. Shannon’s work on the "Entropy of English" was the first time anyone mathematically modeled the probability of the "next word" in a sequence—the very foundation of how I am talking to you right now.
